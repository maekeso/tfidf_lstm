{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 付録\n",
    "今回の分析に使用したPythonのコードを記述する。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 目次\n",
    "1. ディレクトリ構成\n",
    "2. ライブラリのインストール\n",
    "3. データの読み込みと前処理\n",
    "4. 自然言語処理\n",
    "    1. 形態素解析\n",
    "    2. TF-IDFによる文章特徴量の生成\n",
    "5. 主成分分析\n",
    "6. 予測\n",
    "    1. 重回帰分析\n",
    "    2. ランダムフォレスト\n",
    "    3. サポートベクター回帰\n",
    "    4. LSTM\n",
    "7. その他の分析"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ディレクトリ構成\n",
    "ファイルのディレクトリ構成は下記の通りになっている。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>\n",
    "./tfidf_lstm/\n",
    "|- analysis.ipynb #このipynbファイル\n",
    "|\n",
    "|- data/\n",
    "|  |- np_txt/\n",
    "|  |  |- NP1998/\n",
    "|  |  |   |- NP199801.txt #1998年01月の月例経済報告のテキストデータ\n",
    "|  |  |   |- NP199802.txt #1998年02月の月例経済報告のテキストデータ\n",
    "|  |  |   |- ...\n",
    "|  |  |   |- NP199812.txt #1998年12月の月例経済報告のテキストデータ\n",
    "|  |  |\n",
    "|  |  |- NP1999/\n",
    "|  |  |- ...\n",
    "|  |  |- NP2019/\n",
    "|  | \n",
    "|  |- market_data/\n",
    "|     |- nikkei.csv #日経平均株価の時系列データ\n",
    "|\n",
    "|- output/ #分析結果などを出力したディレクトリ\n",
    "|     |- tfidf.csv #tfidfの結果を出力したCSVファイル\n",
    "|     |- model.h5 #LSTMのモデルファイル\n",
    "|\n",
    "|- support_file/\n",
    "   |- stopwords.txt #ストップワードとして扱う単語を収録したテキストデータ\n",
    "\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ライブラリのインストール"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import sklearn\n",
    "import keras\n",
    "import MeCab\n",
    "import glob\n",
    "import re\n",
    "import os\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, TfidfTransformer, CountVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import load_model\n",
    "\n",
    "#豆腐防止用のフォント指定\n",
    "plt.rcParams[\"font.family\"] = \"IPAexGothic\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#使用しているライブラリのバージョンを確認。\n",
    "'''\n",
    "\"Versions of used main library.\n",
    "pandas: 0.25.3\n",
    "numpy: 1.17.4\n",
    "sklearn: 0.22\n",
    "tensorflow: 2.0.0\n",
    "keras: 2.3.1\n",
    "'''\n",
    "\n",
    "print(\"pandas:\",pd.__version__)\n",
    "print(\"numpy:\",np.__version__)\n",
    "print(\"sklearn:\",sklearn.__version__)\n",
    "print(\"tensorflow:\",tf.__version__)\n",
    "print(\"keras:\",keras.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 分析に必要なデータの読み込みと前処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#パスの指定\n",
    "NP_TEXT_PATH = \"./data/np_txt/\"\n",
    "STOPWORDS_PATH = \"./support_file/stopwords.txt\"\n",
    "MARKET_DATA_PATH = \"./data/market_data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#日経平均株価\n",
    "nikkei = pd.read_csv(MARKET_DATA_PATH + \"nikkei.csv\")\n",
    "\n",
    "nikkei.columns = [\"date\", \"close\", \"open\", \"high\", \"low\", \"volume\", \"before_ratio\"]\n",
    "nikkei[\"date\"] = pd.to_datetime(nikkei[\"date\"], format=\"%Y年%m月\")\n",
    "nikkei.set_index(\"date\", inplace=True)\n",
    "\n",
    "#終値\n",
    "close = nikkei[\"close\"].loc[\"1998\":\"2019\"]\n",
    "\n",
    "plt.figure(figsize=(9,4), dpi=100)\n",
    "plt.plot(close)\n",
    "plt.axvline(\"2018\", c=\"red\", alpha=0.4, linewidth=0.7)\n",
    "plt.xlabel(\"年\")\n",
    "plt.ylabel(\"日経平均株価（円）\")\n",
    "plt.legend([\"終値\"])\n",
    "\n",
    "print(nikkei.info())\n",
    "print(nikkei.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ストップワード\n",
    "with open(STOPWORDS_PATH) as f:\n",
    "    stop_words = f.read().split(\"\\n\")\n",
    "    \n",
    "print(\"例：\",stop_words[0:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 自然言語処理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. 形態素解析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#前処理用関数を定義\n",
    "def preprocessing(text):\n",
    "    \n",
    "    \"\"\"\n",
    "    テキストから不要な記号や数字を取り除く。\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    text : str\n",
    "        処理対象の文章。\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    text : str\n",
    "        不要な記号や数字が取り除かれた文章。\n",
    "    \"\"\"\n",
    "    \n",
    "    text = re.sub(r'[0-9]+', '', text)\n",
    "    text = re.sub(r'(https?|ftp)(:\\/\\/[-_\\.!~*\\'()a-zA-Z0-9;\\/?:\\@&=\\+\\$,%#]+)', '' , text)\n",
    "    text = re.sub(r'[【】]', '', text) \n",
    "    text = re.sub(r'[→]', '', text) \n",
    "    text = re.sub(r'[!-~]', '',text)\n",
    "    text = re.sub(r'[︰-＠]', '', text)\n",
    "    text = re.sub(' ', '', text)\n",
    "    text = re.sub(r'\\n+', '\\n', text) \n",
    "    text = re.sub(r'[()]', '', text)\n",
    "    text = text.replace(' ', '')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagger = MeCab.Tagger('-Ochasen -d /usr/local/lib/mecab/dic/mecab-ipadic-neologd')\n",
    "\n",
    "def create_corpus(text):\n",
    "    \n",
    "    \"\"\"\n",
    "    テキストを形態素解析し、スペースを挟んだ単語のリストに変換する。\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    text : str\n",
    "        処理対象の文章。\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    corpus : list of str\n",
    "        単語が' 'を間に挟んだ文章に変換されたリスト\n",
    "    \"\"\"\n",
    "    \n",
    "    sentence = []\n",
    "    \n",
    "    word_info_list = tagger.parse(text).split(\"\\n\")\n",
    "    \n",
    "    for word_info in word_info_list[:-2]:\n",
    "        word_info = word_info.split(\"\\t\")\n",
    "        word = word_info[0] #単語\n",
    "        #yomi = word_info[1] #読み仮名\n",
    "        original = word_info[2] #原型\n",
    "        word_class = word_info[3].split(\"-\")[0] #品詞\n",
    "    \n",
    "        if original not in stop_words and word_class in [\"名詞\"]:\n",
    "            sentence.append(original)\n",
    "            \n",
    "    corpus = \" \".join(sentence)\n",
    "    \n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#パスを指定し、ファイル一覧を取得する。\n",
    "np_txt_path_list =  sorted(glob.glob(NP_TEXT_PATH + \"*/*.txt\"))\n",
    "\n",
    "#テキストを格納する辞書を作成する。\n",
    "text_dict = {}\n",
    "text_list = []\n",
    "year_month_list = []\n",
    "\n",
    "for np_txt_path in np_txt_path_list[0:]:\n",
    "    \n",
    "    #パスから6桁の数値を取得する。(年月)\n",
    "    year_month = re.findall(r'[0-9]{6}', np_txt_path)[0]\n",
    "    year_month_list.append(year_month)\n",
    "    upper = year_month[0:4]\n",
    "    lower = year_month[4:]\n",
    "\n",
    "    #テキストの読み込み\n",
    "    f = open(np_txt_path)\n",
    "    text = f.read()\n",
    "\n",
    "    if upper not in text_dict:\n",
    "        text_dict[upper] = {}\n",
    "    \n",
    "    text = preprocessing(text)\n",
    "    text_list.append(create_corpus(text))\n",
    "    \n",
    "    text_dict[upper][lower] = text "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. TF-IDFによる文章特徴量の生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer()\n",
    "word_corpus = count_vectorizer.fit_transform(text_list)\n",
    "\n",
    "ttf = TfidfTransformer(norm=\"l2\", use_idf=True, smooth_idf=True, sublinear_tf=False)\n",
    "tfidf = ttf.fit_transform(word_corpus)\n",
    "\n",
    "#tfidf\n",
    "tfidf_df = pd.DataFrame(tfidf.toarray(), columns=count_vectorizer.get_feature_names(), index=year_month_list)\n",
    "tfidf_df.index = pd.to_datetime(tfidf_df.index, format='%Y%m')\n",
    "\n",
    "#tf\n",
    "tf_df = pd.DataFrame(word_corpus.toarray(), columns=count_vectorizer.get_feature_names(), index=year_month_list)\n",
    "tf_df.index = pd.to_datetime(tfidf_df.index, format='%Y%m')\n",
    "\n",
    "#idf\n",
    "idf_df = pd.Series(ttf.idf_,index=count_vectorizer.get_feature_names())\n",
    "\n",
    "#保存\n",
    "tfidf_df.to_csv(\"./output/tfidf_result.csv\")\n",
    "\n",
    "tfidf_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 主成分分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#主成分分析\n",
    "pca = PCA(n_components=30)\n",
    "feature = pca.fit_transform(tfidf_df)\n",
    "feature_df = pd.DataFrame(index=tfidf_df.index, columns=[\"comp_\"+str(n) for n in range(1, pca.n_components_+1)], data=feature)\n",
    "feature_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#累積寄与率の可視化\n",
    "cumsum = 0\n",
    "for num ,explained_variance_ratio in enumerate(pca.explained_variance_ratio_):\n",
    "    cumsum += explained_variance_ratio\n",
    "    if cumsum >= 0.75:\n",
    "        print(\"Cumulative contribution:\",cumsum)\n",
    "        print(\"over 0.75 index number:\",num)\n",
    "        break\n",
    "        \n",
    "plt.figure(figsize=(6,4),dpi=100)\n",
    "plt.plot(pca.explained_variance_ratio_.cumsum())\n",
    "\n",
    "plt.axvline(num, linestyle=\"--\",color=\"green\")\n",
    "plt.axhline(0.8, linestyle=\"-.\", color=\"red\")\n",
    "plt.xticks(np.arange(0, 280 + 1, 10), rotation=0)\n",
    "plt.xlim(0,pca.n_components)\n",
    "plt.ylim(0,1)\n",
    "\n",
    "plt.title(\"contribution cumlative rate\")\n",
    "plt.ylabel(\"Contribution rate\")\n",
    "plt.xlabel(\"component\")\n",
    "plt.grid(True)\n",
    "plt.xticks([0, 10, num, 20, 30])\n",
    "\n",
    "plt.legend([\"cumlative rate\", \"over 0.75 index\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 予測 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#データセット作成用の関数を定義\n",
    "def create_dataset(data, feature_columns, target_columns, train_data_length):\n",
    "    \n",
    "    \"\"\"\n",
    "    scikit-learnに対応したデータセットに変換する。\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : pandas.DataFrame\n",
    "        変換対象のデータフレーム\n",
    "    feature_columns : list of str\n",
    "        予測・分類のために使用したい特徴量の列名\n",
    "    target_columns : str\n",
    "        予測・分類したい対象の列名\n",
    "    train_data_length : int\n",
    "        訓練データとして使用するデータの長さ\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    X_train : list\n",
    "        訓練データの特徴量\n",
    "    y_train : list\n",
    "        訓練データの予測・分類対象\n",
    "    X_test : list\n",
    "        テストデータの特徴量\n",
    "    y_test : list\n",
    "        テストデータの予測・分類対象\n",
    "    \"\"\"\n",
    "    \n",
    "    X_train = data[feature_columns].iloc[0:train_data_length].values\n",
    "    X_test = data[feature_columns].iloc[train_data_length:].values\n",
    "    y_train = data[target_columns].iloc[0:train_data_length].values\n",
    "    y_test = data[target_columns].iloc[train_data_length:].values\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mergeするDataFrameはtfidf_df or feature_dfを選択する。\n",
    "merge_df = feature_df.iloc[:,0:num].join(close)\n",
    "\n",
    "minmax = MinMaxScaler(feature_range=(0,1))\n",
    "merge_df[:] = minmax.fit_transform(merge_df)\n",
    "\n",
    "#今回は訓練データの長さを200、テストデータの長さを残りの64にした。\n",
    "X_train, y_train, X_test, y_test = create_dataset(merge_df, \n",
    "                                                  merge_df.columns[:-1], \n",
    "                                                  merge_df.columns[-1], \n",
    "                                                  train_data_length=240)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. 重回帰分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "lr_pred = lr.predict(X_test)\n",
    "\n",
    "print(\"mae:\",mean_absolute_error(y_test, lr_pred))\n",
    "print(\"mse:\",mean_squared_error(y_test, lr_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. サポートベクター回帰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svr = SVR(C=800, epsilon=0.007,kernel='rbf', gamma='auto')\n",
    "svr.fit(X_train, y_train)\n",
    "svr_pred = svr.predict(X_test)\n",
    "\n",
    "print(\"mae:\",mean_absolute_error(y_test, svr_pred))\n",
    "print(\"mse:\",mean_squared_error(y_test, svr_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. ランダムフォレスト"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestRegressor(n_estimators=100, max_depth=None, random_state=0, criterion=\"mse\")\n",
    "rf.fit(X_train, y_train)\n",
    "rf_pred = rf.predict(X_test)\n",
    "\n",
    "print(\"mae:\",mean_absolute_error(y_test, rf_pred))\n",
    "print(\"mse:\",mean_squared_error(y_test, rf_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D. LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#データ整形用の関数\n",
    "def prepare_data_for_lstm(df, feature_columns, target_column, train_data_length, window_size, normalized=False):\n",
    "    \n",
    "    \"\"\"\n",
    "    kerasに対応したデータセットに変換する。\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas.DataFrame\n",
    "        変換対象のデータフレーム\n",
    "    feature_columns : list of str\n",
    "        予測・分類のために使用したい特徴量の列名\n",
    "    target_columns : str\n",
    "        予測・分類したい対象の列名\n",
    "    train_data_length : int\n",
    "        訓練データとして使用するデータの長さ\n",
    "    window_size : int\n",
    "        時間窓の長さ\n",
    "    normalized : bool\n",
    "        Falseである場合、正規化を行う。Trueである場合行わない。\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    X_train : list\n",
    "        訓練データの特徴量\n",
    "    y_train : list\n",
    "        訓練データの予測・分類対象\n",
    "    X_test : list\n",
    "        テストデータの特徴量\n",
    "    y_test : list\n",
    "        テストデータの予測・分類対象\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"feature:\",feature_columns)\n",
    "    print(\"target:\", target_column)\n",
    "    \n",
    "    feature_columns.append(target_column)\n",
    "    columns = feature_columns\n",
    "    values = df[columns].values\n",
    "    \n",
    "    if normalized == False:\n",
    "        sc = MinMaxScaler()\n",
    "        values = sc.fit_transform(values)\n",
    "        \n",
    "    #create dataset\n",
    "    dataset = []\n",
    "    for t in range(0, len(df) - window_size):\n",
    "        dataset.append(values[t:t+window_size])\n",
    "    dataset = np.asarray(dataset)\n",
    "    \n",
    "    print(\"shape of dataset:\",dataset.shape)\n",
    "    \n",
    "    #split data \n",
    "    X_train = dataset[0:train_data_length, :, :-1] \n",
    "    X_test = dataset[train_data_length:,: ,:-1]\n",
    "    y_train = values[window:window+train_data_length, -1]\n",
    "    y_test = values[window+train_data_length:, -1]\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#window + train_size = 240になるように設定する。\n",
    "window = 72\n",
    "train_size = 168 #len(merge_df) - window - 12\n",
    "target_name = \"close\"\n",
    "\n",
    "print(train_size)\n",
    "\n",
    "merge_df = feature_df.iloc[:, 0:12].join(close)\n",
    "\n",
    "X_train, y_train, X_test, y_test = prepare_data_for_lstm(merge_df,\n",
    "                                                         feature_columns=merge_df.columns[:-1].tolist(),\n",
    "                                                         target_column=target_name,\n",
    "                                                         train_data_length=train_size,\n",
    "                                                         window_size=window,\n",
    "                                                         normalized=False)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#モデルを読み込む場合(推奨)\n",
    "model = keras.models.load_model('./output/model.h5', compile=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## モデル構築(予め出力しているモデルのデータがあるので、そちらの読み込みを推奨)\n",
    "\n",
    "#乱数の固定\n",
    "np.random.seed(1)\n",
    "tf.random.set_seed(1)\n",
    "\n",
    "#LSTMのモデル構築。\n",
    "#今回はLSTM層と全結合層を一つずつ使うモデルを構築した。\n",
    "model = Sequential() \n",
    "model.add(LSTM(units=1024,\n",
    "               input_shape=(window, len(merge_df.columns)-1), \n",
    "               return_sequences=False, \n",
    "               ))\n",
    "model.add(Dense(units=1, \n",
    "                activation=\"linear\"))\n",
    "\n",
    "#最適化アルゴリズムにRMSpropを使用。\n",
    "#学習率は過学習防止のため1e-5にしている。\n",
    "optimizer = RMSprop(learning_rate=1e-4)\n",
    "model.compile(loss=\"mse\", optimizer=optimizer)\n",
    "\n",
    "#学習\n",
    "model.fit(x=X_train, y=y_train, batch_size=32, epochs=1000, verbose=2, validation_split=0.25,\n",
    "          callbacks=[EarlyStopping(monitor='val_loss', patience=5, verbose=2, mode='auto')])\n",
    "\n",
    "print(\"complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LSTMの精度の計算\n",
    "print(\"mae\", mean_absolute_error(y_test, model.predict(X_test).ravel()))\n",
    "print(\"mse\", mean_squared_error(y_test, model.predict(X_test).ravel()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#モデルの保存\n",
    "model.save('./output/model.h5', include_optimizer=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. その他の分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#各主成分における寄与率を計算する。\n",
    "comp_df = pd.DataFrame(pca.components_.T, columns=[\"主成分\" + str(n) for n in range(1,pca.n_components_+1)], index=tfidf_df.columns)\n",
    "\n",
    "factor_loading = pd.DataFrame(index=range(1,11))\n",
    "for col_name, item in comp_df.iteritems():\n",
    "    factor_loading[col_name] = abs(item).sort_values(ascending=False)[0:10].index\n",
    "    \n",
    "factor_loading.iloc[:, 0:30].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#それぞれの因子負荷量のランキングを表示する。\n",
    "factor_loading.iloc[:, 0:10].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factor_loading.iloc[:, 10:20].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factor_loading.iloc[:, 20:30].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#特徴重要度の可視化\n",
    "plt.figure(figsize=(5,4),dpi=100)\n",
    "rf_feature_importance = pd.Series(rf.feature_importances_, index=feature_df.iloc[:,0:rf.n_features_].columns)\n",
    "rf_feature_importance.sort_values(ascending=False).plot(kind=\"bar\", title=\"feature importance\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#総単語数の可視化\n",
    "plt.figure(figsize=(9,4), dpi=100)\n",
    "tf_df.sum(axis=1).plot()\n",
    "plt.xlabel(\"年\")\n",
    "plt.ylabel(\"総単語数（個）\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#重回帰分析の回帰係数\n",
    "print(\"intercept:\",lr.intercept_)\n",
    "print(pd.Series(lr.coef_, index=merge_df.columns[:-1], name=\"linear_coef\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss, validation_lossの推移を可視化する。(用意したモデルを読み込んだ場合はエラーが起こるので可視化の際には再度学習が必要となる。)\n",
    "plt.plot(model.history.history[\"loss\"])\n",
    "plt.plot(model.history.history[\"val_loss\"])\n",
    "plt.legend(labels=[\"loss\", \"val_loss\"])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
